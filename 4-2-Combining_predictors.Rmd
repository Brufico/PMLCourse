---
title: "Practical Machine Learning - Week 4 - 2 - Combining Predictors"
subtitle: 
author:  "Jeff Leek, notes by Bruno Fischer Colonimos"
date: "`r format(Sys.Date(), '%d %B %Y')`"
fontsize: 12pt
urlcolor: blue
linkcolor: red
documentclass: article
classoption: a4paper
geometry: "left=1.5cm,right=1.5cm,top=1.5cm,bottom=2cm,footskip=1cm"
output:
  pdf_document: 
    highlight: monochrome
    number_sections: yes
    toc: yes
    toc_depth: 4
    
  html_document: 
    number_sections: yes
    theme: readable
    toc: yes
---

----------------

# Intro {sec:intro}


This lecture is about combining predictors, it's sometimes called ensembling methods in learning.   



# Key ideas

* You can combine classifiers by averaging/voting
* Combining classifiers improves accuracy. (In general combining classifiers together improves accuracy)
* Combining classifiers reduces interpretability. you have to be really careful the gain that you get is worth the loss on interpretability
* Boosting, bagging, and random forests are variants on this theme, But those are all examples where it's the *same kind* of classifiers being averaged in every case.

---

# Example: Netflix prize

The team that ended up wining had actually a blended *combination of **107** different Machine learning algorithms* combined together in order to get the best score in this example.(BellKor) = Combination of 107 predictors 


[http://www.netflixprize.com//leaderboard](http://www.netflixprize.com//leaderboard)

---

## Example: Heritage health prize - Progress Prize 1

The Heritage Health prize was a $3 million prize. It was designed to try to predict whether people would go back to the hospital based on their hospitalization record. 

The Heritage Health prize was also won by methods that were ensembling for multiple different regression models and machine learning methods. And so this was these are both of the links to the first progress prizes for the two teams that were in the lead. 

[Market Makers](https://kaggle2.blob.core.windows.net/wiki-files/327/e4cd1d25-eca9-49ca-9593-b254a773fe03/Market%20Makers%20-%20Milestone%201%20Description%20V2%201.pdf)


[Mestrom](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)



## Basic intuition - majority vote

Suppose we have 5 completely independent classifiers

If accuracy is 70% for each:
* $\binom{5}{3}\times(0.7)^3(0.3)^2 + \binom{5}{4}\times(0.7)^4(0.3)^2 + (0.7)^5 = 0.887 $
    * 83.7% majority vote accuracy

With 101 independent classifiers
    * 99.9% majority vote accuracy
  

---

## Approaches for combining classifiers

1. Bagging, boosting, random forests
  * Usually combine similar classifiers
2. Combining different classifiers
  * Model stacking
  * Model ensembling
We're going to talk a little bit about model stacking in this lecture  





## Example with Wage data

__Create training, test and validation sets__

```{r wage}
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))

# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]

# from the buildig data set, create a training set and a test set
inTrain <- createDataPartition(y=buildData$wage,
                              p=0.7, list=FALSE)
training <- buildData[inTrain,]
testing <- buildData[-inTrain,]

# verification

dim(training)
dim(testing)
dim(validation)
```



---

## Build two different models

On the training set, let us build: 

* a generalized linear model
* a random forest model

```{r modFit,dependson="wage", cache = TRUE}
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf", # random forest
              data = training, 
              trControl = trainControl(method="cv"),number=3)
```


---

## Predict on the testing set 

I can then plot those predictions versus each other. Pred1 is the predictions from the linear model, and pred2 are the predictions from the random forest. And you can see that they, they're close to each other, but they don't actually agree with each other. And, neither of them perfectly correlates with the wage variable, which is the color of the dots in this particular picture. 

```{r predict,dependson="modFit",fig.height=4,fig.width=6}
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
```




## Fit a model that combines predictors

 I basically build a new data set consisting of the predictions (on the test set) from model one and from model two, and the variable wage.
 
Then I can fit a new model, which relates this wage variable to the two predictions. (So now, instead of just fitting a model that relates the covariates to the outcome, I've fit two separate prediction models for the outcome. And now now I'm fitting a regression model, relating the outcome to the predictions for those two models. And then I can predict from the combined data set on new samples. 

```{r combine,dependson="predict", cache=TRUE}
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
```



## Testing errors (testing set)

```{r ,dependson="combine"}
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
```



## Predict on validation data set

```{r validation,dependson="combine"}
pred1V <- predict(mod1,validation)
pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
# use the model previously fitted
```



## Evaluate on validation

```{r ,dependson="validation"}
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
```
The combined model has a lower error even on the validation data set. So stacking models in this way can  improve accuracy by blending different models, the strengths of different models together. Even simple blending like I've talked about here can be really useful and can improve accuracy. 



# Some graphs

```{r}
df <- data.frame(pred = combPredV,
                 truth = validation$wage)

ggplot(df, aes(pred, truth)) + geom_point()

```

---

## Notes and further resources

* Even simple blending can be useful
* Typical model for binary/multiclass data
  * Build an odd number of models
  * Predict with each model
  * Predict the class by majority vote
* This can get dramatically more complicated
  * Simple blending in caret: [caretEnsemble](https://github.com/zachmayer/caretEnsemble) (use at your own risk!)
  * Wikipedia [ensemble learning](http://en.wikipedia.org/wiki/Ensemble_learning)

---

## Recall - scalability matters

One important point to keep in mind when doing model ensembling is that this can lead to increases in computational complexity. So it turns out that even though Netflix paid a million dollars to the team that won the prize. The solution, the Netflix millionaire dollar solution was never actually implemented. Because it was too computational intensive to apply to specific data sets

<img class=center src=../../assets/img/08_PredictionAndMachineLearning/netflixno.png height=250>
</br></br></br>

[http://www.techdirt.com/blog/innovation/articles/20120409/03412518422/](http://www.techdirt.com/blog/innovation/articles/20120409/03412518422/)

[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)
