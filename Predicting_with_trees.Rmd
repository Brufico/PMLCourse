---
title: "Practical Machine Learning - Week 3 - Predicting with trees"
subtitle: 
author:  "Jeff Leek, notes by Bruno Fischer Colonimos"
date: "`r format(Sys.Date(), '%d %B %Y')`"
urlcolor: blue
geometry: top=0.5in, left=0.6in, right=0.6in, bottom = 0.7in, footskip = 0.3in
output:
  pdf_document: 
    number_sections: yes
    toc: yes
  html_document: 
    number_sections: yes
    theme: readable
    toc: yes
---

----------------

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file


getlib <- function(libname) {
        if (!require(libname, character.only = TRUE)) {
                install.packages(libname)
                library(libname, character.only = TRUE)

        }
}


library(knitr)
getlib("xtable") #does not always work....

options(width = 100)

opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center',  cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
# knit_hooks$set(inline = function(x) {
#   if(is.numeric(x)) {
#     round(x, getOption('digits'))
#   } else {
#     paste(as.character(x), collapse = ', ')
#   }
# })
# knit_hooks$set(plot = knitr:::hook_plot_html) # with this hook, pdf does not render plots. BFC
```

```{r libs,include=FALSE}
# library(caret)
if (!require("caret", character.only = TRUE)) {
        install.packages("caret")
        library("caret", character.only = TRUE)
        
}

getlib("rattle")
# install.packages('rpart.plot') # must be installed (once)
library(grid)
library(gridExtra)

```



Key ideas
=========

* Iteratively split variables into groups
* Evaluate "homogeneity" within each group
* Split again if necessary

__Pros__:

* Easy to interpret
* Better performance in nonlinear settings

__Cons__:

* Without pruning/cross-validation can lead to overfitting
* Harder to estimate uncertainty
* Results may be variable


---

Example Tree (Obama/Clinton)
----------------------------

[http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg](http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)

---

Basic algorithm
===============

1. Start with all variables in one group
2. Find the variable/split that best separates the outcomes
3. Divide the data into two groups ("leaves") on that split ("node")
4. Within each split, find the best variable/split that separates the outcomes
5. Continue until the groups are too small or sufficiently "pure"


---

Measures of impurity
====================

__Probability that an object of the mth leaf is part of the kth class__

Here are different measures of impurity. They're all based on this probability that you can estimate: p hat for the mth leaf and the kth class:\
In the **m** leaf  you have $N_m$ total objects that you might consider, and you can count the number of times that a particular class **k** appears in that leaf. So p hat is the number of times that class k appears in leaf m divided by $N_m$ :

$$\hat{p}_{mk} = \frac{1}{N_m}\sum_{x_i\; in \; Leaf \; m}\mathbb{1}(y_i = k)$$

__Misclassification Error__: 

The misclassification error is 1 minus the probability that you're equal to the most common class in that particular leaf. So for example, if it's a leaf where almost all of the counties would vote for Barack Obama, then 1 minus the misclassification error is 1 minus the probability that you'd vote for Barack Obama. 
$$ 1 - \hat{p}_{m k(m)} \quad ; \quad k(m) = {\rm most \  common \ k}$$ 
Values:

* 0 = perfect purity
* 0.5 = no purity

__Gini index__:

The Gini index (which is not to be confused with the Gini coefficient in economics). (*is it for 2 classes only?? No, I guess not*)\
It's basically 1 minus the sum of the squared probabilities that you belong to any of the different classes. 

$$ \sum_{k \neq k'} \hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) = 1 - \sum_{k=1}^K p_{mk}^2$$

Values: So again zero means perfect purity. In other words the class has one particular class which has probability equal to 1 and all the other classes have probability equal to 0. 0.5 means no purity. In other words, all of the classes are perfectly balanced within each leaf. 

* 0 = perfect purity
* 0.5 = no purity

http://en.wikipedia.org/wiki/Decision_tree_learning


__Deviance/information gain__:

Deviance and information gain is another measure that can be used. It's called **deviance** if you use log with base e here, and with log base 2 it is **information gain**. And it's basically minus the sum of the probability of being assigned to class **k** (*or **belonging** to class k?*) in leaf **m** times the log (base 2 or base e), of that same probability. 

$$-\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk} $$
**Values**

* 0 = perfect purity
* 1 = no purity

**More**: http://en.wikipedia.org/wiki/Decision_tree_learning



**After [wikipedia](http://en.wikipedia.org/wiki/Decision_tree_learning): alternate take on Information gain**

* **Entropy**


$$H = -\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk}$$

* **Information gain of a tree split**

$$Information \; Gain = Entropy(parent) - \mathrm{Weighted \; Sum \; of \;} (Entropy(Children))$$

See example in Wikipedia





Measures of impurity, Example
-----------------------------


```{r leftplot1, fig.width=5, fig.asp= 0.76, echo=FALSE, cache = FALSE}


ga <- ggplot(data = data.frame(x = rep(1:4,each = 4), y = rep(1:4,4), 
                               classe = factor(c(rep(1,15), rep(2,1)))),
             aes(x, y, colour = classe, shape = factor(classe))) + 
        geom_point(size = 4) + 
        scale_x_continuous(name = "", breaks = NULL) +
        scale_y_continuous(name = "", breaks = NULL) +
        scale_shape_discrete(name = "Classe") +
        scale_color_discrete(name = "Classe") +
        labs(title = "A") + 
        theme(plot.background = element_rect(colour="black", size = 1),
              plot.title = element_text(hjust = 0.5))

```

```{r rightplot2,  fig.width=5, fig.asp= 0.76, echo=FALSE, cache = FALSE}

gb <- ggplot(data = data.frame(x = rep(0.5:3.5,each = 4), y = rep(1:4,4), 
                               classe = factor(c(rep(1,8), rep(2,8)))),
             aes(x, y, colour = classe, shape = classe)) + 
        geom_point(size = 4) + 
        scale_x_continuous(name = "", breaks = NULL) +
        scale_y_continuous(name = "", breaks = NULL) +
        scale_shape_discrete(name = "Classe") +
        scale_color_discrete(name = "Classe") +
        labs(title = "B") + 
        theme(plot.background = element_rect(colour="black", size = 1),
              plot.title = element_text(hjust = 0.5))
```

```{r show2plots, fig.width=10, echo=FALSE, cache = FALSE, fig.width=10, fig.asp= 0.38 , fig.align = 'center', out.width="\\textwidth"}
grid.newpage()
grid.arrange(ga, gb, ncol = 2)
```


* **case A**
    * **Misclassification:** $1/16 = 0.06$
    * **Gini:** $1 - [(1/16)^2 + (15/16)^2] = 0.12$
    * **Information** $-[1/16 \times log2(1/16) + 15/16 \times log2(15/16)] = 0.34$

* **case B**
    * **Misclassification:** $8/16 = 0.5$
    * **Gini:** $1 - [(8/16)^2 + (8/16)^2] = 0.5$
    * **Information** $-[1/16 \times log2(1/16) + 15/16 \times log2(15/16)] = 1$


---


Example: Iris Data
------------------

```{r iris, cache=TRUE}
data(iris); library(ggplot2)
names(iris)
kable(t(as.matrix(table(iris$Species))))
```


---

## Create training and test sets

```{r trainingTest, dependson="iris",cache=TRUE}
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```


---

## Iris petal widths/sepal width and petal lengths/sepal lengths

```{r, dependson="trainingTest",fig.width=5, fig.asp= 0.75 , out.width="0.48\\textwidth"}
# removed: fig.height=4,
gwidth <- ggplot(data = training, aes(Petal.Width, Sepal.Width, colour=Species)) + geom_point()+ 
        theme(plot.background = element_rect(colour="black", size = 1),
              plot.title = element_text(hjust = 0.5))
glen <- ggplot(data = training, aes(Petal.Length, Sepal.Length, colour=Species)) + geom_point()+ 
        theme(plot.background = element_rect(colour="black", size = 1),
              plot.title = element_text(hjust = 0.5))
```


```{r, dependson="trainingTest", fig.width=10, fig.asp= 0.38 , fig.align = 'left', out.width="\\textwidth", echo = FALSE}
grid.newpage()
grid.arrange(gwidth, glen, ncol = 2) 

```

---

## Tree creation, Iris petal widths/sepal width

```{r createTree, dependson="trainingTest", cache=TRUE}
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
# summary(modFit$finalModel)

# another try
modFit2 <- train(Species ~ Petal.Length + Sepal.Length,method="rpart",data=training)
print(modFit2$finalModel)
 
```

---


## Plot tree

```{r, dependson="createTree", fig.height=4.5, fig.width=4.5}
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.7)
```


---

## Prettier plots

```{r, dependson="createTree", fig.height=5.5, fig.width=4}
library(rattle)
fancyRpartPlot(modFit$finalModel, main = "Classification Tree")
```

```{r, dependson="createTree", fig.height=5.5, fig.width=4}
# or (BFC try)
fancyRpartPlot(modFit2$finalModel, main = "Classification Tree 2")

```

---


## Predicting new values

```{r newdata, dependson="createTree", fig.height=4.5, fig.width=4.5, cache=TRUE}
predtest <- predict(modFit,newdata=testing)
cm1 <- confusionMatrix(predtest,testing$Species)
# a voir
# cm1$table
# cm1$overall
# cm1$byClass

cm1
```
```{r}
# model2
confusionMatrix(predict(modFit2,newdata=testing),
                testing$Species)
```



---

## Notes and further resources

* Classification trees are non-linear models
    * They use interactions between variables
    * Data transformations may be less important (monotone transformations)
    * Trees can also be used for regression problems (continuous outcome)
* Note that there are multiple tree building options in R both 
    * in the caret package 
        - [party](http://cran.r-project.org/web/packages/party/index.html), [rpart](http://cran.r-project.org/web/packages/rpart/index.html) 
    * and out of the caret package 
        - [tree](http://cran.r-project.org/web/packages/tree/index.html)

* More: 
    * [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
    * [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
    * [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)


